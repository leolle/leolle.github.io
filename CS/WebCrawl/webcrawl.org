#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t c:nil
#+OPTIONS: creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t
#+OPTIONS: num:t p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t
#+OPTIONS: title:t toc:t todo:t |:t
#+TITLEs: webcrawl
#+DATE: <2018-04-13 Fri>
#+AUTHORs: weiwu
#+EMAIL: victor.wuv@gmail.com
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 24.5.1 (Org mode 8.3.4)
#+SETUPFILE: ../../configOrg/level2.org
* extract data from the HTML source

** beautifulsoup
BeautifulSoup is a very popular web scraping library among Python programmers which constructs a Python object based on the structure of the HTML code and also deals with bad markup reasonably well, but it has one drawback: it’s slow.
#+BEGIN_SRC python
from bs4 import BeautifulSoup

html = '''
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
'''

soup = BeautifulSoup(html,'lxml')
# title
print(soup.title.text)
print(soup.find_all('p'))

# class
print(soup.title.name)
print(soup.title.string)
print(soup.title.parent.name)
print(soup.p)
print(soup.p["class"])
print(soup.a)
print(soup.find_all('a'))
print(soup.find(id='link3'))
print(soup.a['href'])
# find href
soup.find('p').attr('href')
#+END_SRC

** selenium

#+BEGIN_SRC python
import sys

import requests
import time
import datetime
import random
import pandas as pd

from lxml import etree
from selenium import webdriver
from tqdm import tqdm
import json

from selenium.webdriver.common.desired_capabilities import DesiredCapabilities

caps = DesiredCapabilities.FIREFOX
caps['loggingPrefs'] = {'performance': 'ALL'}
driver = webdriver.PhantomJS('/home/wuwei/projects/text_data_mining/catch_wechat/phantomjs-2.1.1-linux-x86_64/bin/phantomjs', desired_capabilities=caps)
driver.get('http://spds.qhrb.com.cn/SP12/SPOverSee1.aspx')
table = driver.find_element_by_css_selector('#form1 > div.main > table')
tr = table.find_elements_by_tag_name('tr')
len(tr[0].find_elements_by_tag_name('td'))
tr[1].find_elements_by_tag_name('td')[1].text
data = table.find_elements_by_css_selector('#form1 > div.main > table > tbody')
# read page content into pandas
html = driver.page_source
pd.read_html(html)[1]

# jump to the next page
fp_next = driver.find_element_by_xpath('//*[@id="AspNetPager1"]/a[12]')
fp_next.click()
#+END_SRC

- login
#+BEGIN_SRC python
from selenium import webdriver
import time

driver = webdriver.Firefox()
driver.get("https://www.baidu.com/")
time.sleep(3)
driver.find_element("userName").send_keys("1356046")
driver.find_element("password").send_keys("x9122")
driver.find_element("linkText","登录").click()

cookies = driver.get_cookies()
print (type(cookies))
# print ("".join(cookies))
f1 = open('cookie.txt', 'w')
f1.write(json.dumps(cookies))
f1.close

f1 = open('cookie.txt')
cookie = f1.read()
cookie =json.loads(cookie)
for c in cookie:
    driver.add_cookie(c)
# # 刷新页面
driver.refresh()
#+END_SRC

** lxml
lxml is an XML parsing library (which also parses HTML) with a pythonic API based on ElementTree. (lxml is not part of the Python standard library.)

* Scrapy
** Constructing selectors
Scrapy selectors are instances of Selector class constructed by passing text or TextResponse object. It automatically chooses the best parsing rules (XML vs HTML) based on input type:

>>> from scrapy.selector import Selector
>>> from scrapy.http import HtmlResponse
Constructing from text:

>>> body = '<html><body><span>good</span></body></html>'
>>> Selector(text=body).xpath('//span/text()').extract()
[u'good']
** Constructing from response:

>>> response = HtmlResponse(url='http://example.com', body=body)
>>> Selector(response=response).xpath('//span/text()').extract()
[u'good']
For convenience, response objects expose a selector on .selector attribute, it’s totally OK to use this shortcut when possible:

>>> response.selector.xpath('//span/text()').extract()
[u'good']

* Create a new scrapy project
#+BEGIN_SRC bash
scrapy startproject tutorial
#+END_SRC

- directory content:
#+BEGIN_SRC txt
└── quotes
    ├── quotes
    │   ├── __init__.py
    │   ├── items.py
    │   ├── middlewares.py
    │   ├── pipelines.py
    │   ├── __pycache__
    │   ├── settings.py
    │   └── spiders
    │       ├── __init__.py
    │       └── __pycache__
    └── scrapy.cfg
#+END_SRC
- save a python file under quotes/spiders.
- the name of the crawler should be defined in class name as unique.
- save the code for our spider under *quotes/quotes/spider*.
- the name of the spider is defined uniquely in the class name.
#+BEGIN_SRC python
class QuotesSpider(scrapy.Spider):
    name = "quotes" # unique name

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
#+END_SRC

** input

1. 定义item
Item 是保存爬取到的数据的容器；这里相当于定义需要爬哪些内容，如title, desc, date.

2. start_urls
·         start_urls: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。

3. 提取Item selectors选择器（重点）
从网页中提取数据有很多方法。Scrapy使用了一种基于 XPath 和 CSS 表达式机制: Scrapy Selectors.
这里给出XPath表达式的例子及对应的含义:
·         /html/head/title: 选择HTML文档中 <head> 标签内的 <title> 元素
·         /html/head/title/text(): 选择上面提到的 <title> 元素的文字
·         //td: 选择所有的 <td> 元素
·         //div[@class="mine"]: 选择所有具有 class="mine" 属性的 div 元素
Selector有四个基本的方法:
·         xpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表 。
·         css(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表.
·         extract(): 序列化该节点为unicode字符串并返回list。
·         re(): 根据传入的正则表达式对数据进行提取，返回unicode字符串list列表。

4. setting配置文件
针对不同的网站，可能需要不同setting。
在这里可以配置输出的方式，如csv, json等。

** extracting data test in shell

#+BEGIN_SRC bash
scrapy shell https://www.jianshu.com/p/fa614bea98eb
#+END_SRC
you can try selecting elements using CSS or xpath with the response object
#+BEGIN_SRC bash
In [31]: books = response.xpath('//div[@class="bd doulist-subject"]').extract()

In [32]: books
#+END_SRC

#+RESULT:
: ['<div class="bd doulist-subject">\n    \n\n\n\n    <div class="source">\n      来自：豆瓣读书\n    </div>\n    \n    <div class="post">\n      <a href="https://book.douban.com/subject/10519369/" target="_blank">\n        <img width="100" src="https://img1.doubanio.com/view/subject/l/public/s8869768.jpg">\n      </a>\n    </div>\n    <div class="title">\n      <a href="https://book.douban.com/subject/10519369/" target="_blank">\n        万物生光辉\n      </a>\n    </div>\n    \n      <div class="rating">\n          <span class="allstar45"></span>\n          <span class="rating_nums">9.4</span>\n          <span>(1052人评价)</span>\n      </div>\n    <div class="abstract">\n      \n          作者: [英] 吉米·哈利\n            <br>\n          出版社: 中国城市出版社\n            <br>\n          出版年: 2012-3\n    </div>\n  </div>',...]


** run the program

- store the scraped data
#+BEGIN_SRC bash
cd webcrawl/scrapy/quotes
#+END_SRC
#+BEGIN_SRC python
scrapy crawl quotes -o quotes.json
# or
scrapy crawl quotes -o quotes-humor.json -a tag=humor
#+END_SRC

** proxy
- ip proxy tool
https://github.com/qiyeboy/IPProxyPool
#+BEGIN_SRC python
import requests
import json
r = requests.get('http://192.168.4.36:8555/?types=0&count=5&country=国内')
ip_ports = json.loads(r.text)
print ip_ports
ip = ip_ports[0][0]
port = ip_ports[0][1]
proxies={
    'http':'http://%s:%s'%(ip,port),
    'https':'http://%s:%s'%(ip,port)
}
r = requests.get('http://ip.chinaz.com/',proxies=proxies)
r.encoding='utf-8'
#+END_SRC

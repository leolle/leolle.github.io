<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-09-30 Mon 10:36 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>&lrm;</title>
<meta name="generator" content="Org mode">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="http://fonts.googleapis.com/css?family=Roboto+Slab:400,700|Inconsolata:400,700" rel="stylesheet" type="text/css" />
<link href="css/style.css" rel="stylesheet" type="text/css" />
<link rel="stylesheet" type="text/css" href="./styles/demo/css/style.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<div class="outline-2" id="meta">
<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left"><b>Author</b></td>
<td class="org-left">Wei Wu (victor.wuv@gmail.com)</td>
</tr>

<tr>
<td class="org-left"><b>Date</b></td>
<td class="org-left">2019-09-30 10:36:44</td>
</tr>
</tbody>
</table>
</div>


<div id="outline-container-org43a1d3d" class="outline-2">
<h2 id="org43a1d3d">Workflow</h2>
<div class="outline-text-2" id="text-org43a1d3d">
</div>
<div id="outline-container-org088ac29" class="outline-3">
<h3 id="org088ac29">quant</h3>
<div class="outline-text-3" id="text-org088ac29">
<ol class="org-ol">
<li>define goal.</li>
<li>search paper/report.</li>
<li>read pdf/code, take notes.</li>
<li>present general idea.</li>
<li>write pseudo code, set input parameters.</li>
<li>write/modify code.</li>
<li>plugging gs to run with data.</li>
<li>check out result.</li>
<li>prepare presentation of all previous steps.</li>
<li>present and get feedback.</li>
<li>deploy function definition, function sampling.</li>
</ol>
</div>
</div>

<div id="outline-container-org7c97a07" class="outline-3">
<h3 id="org7c97a07">data analysis</h3>
<div class="outline-text-3" id="text-org7c97a07">
<ol class="org-ol">
<li>分析</li>
<li>实验仿真</li>
<li>可视化</li>
<li>建模</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orgbd67f81" class="outline-2">
<h2 id="orgbd67f81">Deep Learning</h2>
<div class="outline-text-2" id="text-orgbd67f81">
</div>
<div id="outline-container-org8ea9785" class="outline-3">
<h3 id="org8ea9785">Natural Language Processing</h3>
<div class="outline-text-3" id="text-org8ea9785">
</div>
<div id="outline-container-org2b1452a" class="outline-4">
<h4 id="org2b1452a"><span class="done DONE">DONE</span> entity relationship extraction <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-19 Mon&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-21 Wed&gt;</span></span></h4>
<div class="outline-text-4" id="text-org2b1452a">
<ul class="org-ul">
<li>命名实体识别</li>
</ul>
<p>
现在常用的方法有「LSTM + 条件随机场（CRF）」、「最大熵隐马尔科夫」、「隐马尔科夫」等序列标注模型。 主要的处理思想有:
</p>

<ul class="org-ul">
<li class="on"><code>[X]</code> finish join learning entity extraction paper.<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-20 Tue&gt;</span></span></li>
<li class="off"><code>[&#xa0;]</code> summerize text first, then event extraction?</li>
<li class="on"><code>[X]</code> find source code and scheme for this paper.</li>
<li class="off"><code>[&#xa0;]</code> from survey paper -&gt; book -&gt; reference paper -&gt; citation paper -&gt; application -&gt; open source library.</li>
<li class="off"><code>[&#xa0;]</code> company relation</li>
<li class="off"><code>[&#xa0;]</code> analyst relation</li>
<li class="off"><code>[&#xa0;]</code> entity disambiguation: extraction resolution detection like author, publisher.</li>
<li class="on"><code>[X]</code> pseudo code of node, edge upload.</li>
<li class="on"><code>[X]</code> summerize nlp library extraction result comparison in jupyter notebook.</li>
<li class="on"><code>[X]</code> find the difference of attirbutes not in Juyuan database, searching for useful information.</li>
</ul>
<p>
聚源数据库已经包含了大量的公司信息，暂时没有在百科三元组发现更有价值的信息。
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> extract the triple relation information.</li>
<li class="on"><code>[X]</code> visualization of triples.</li>
<li class="off"><code>[&#xa0;]</code> NER of all listed company pages content what analyst care about: 有关内容包括：主要产品，产业链，竞争对手，合作伙伴，投资方，key person(如公司跟投资人关联), 上市交易所，sentiment, 分析师评级，评论，公司重大公告.</li>
<li class="on"><code>[X]</code> Chinese NER model is missing, searching. models are in the Chinese model jar file.</li>
<li class="on"><code>[X]</code> test stanford-corenlp to extract keywords and NER en.</li>
<li class="on"><code>[X]</code> compare nlp libraries.</li>
<li class="on"><code>[X]</code> extract Named Entity Recognition.</li>
<li class="off"><code>[&#xa0;]</code> extract RDF company triples.</li>
<li class="off"><code>[&#xa0;]</code> listed companies triples importing to neo4j.</li>
<li class="off"><code>[&#xa0;]</code> read Q&amp;A knowledge graph paper.</li>
</ul>
</div>

<div id="outline-container-org2d749b8" class="outline-5">
<h5 id="org2d749b8"><span class="done DONE">DONE</span> 语料收集:<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-21 Wed&gt;</span></span></h5>
<div class="outline-text-5" id="text-org2d749b8">
<ul class="org-ul">
<li>目标语料格式：</li>
</ul>
<p>
实体1  实体2  关系  包括实体1，实体2和他们之间关系的语句。
</p>
<ul class="org-ul">
<li>加快语料收集的想法：
<ol class="org-ol">
<li>自定义字典法，利用已有的种子实体。</li>
<li>在SSE上搜索已经有的投资，收购等种子实体关系，得到语料。</li>
<li>利用NER<sub>IDCNN</sub><sub>CRF的实体识别得到语料里面的实体</sub>，现有模型支持人名，组织机构和位置。</li>
<li>从distant supervision的方法中获取灵感，可以首先找到具有确定关系的实体对，然后再去爬取该实体对共同出现的语句作为正样本。负样本则从实体库中随机产生没有关系的实体对，最后去爬取这样实体对共同出现的语句，这样的语句可以通过网络爬虫从雪球，google news抓取。*这样保证了语料收集的快速性和关系数量的扩展性*。</li>
<li>对于具有确定关系的实体对，从百度百科Triples得到。</li>
</ol></li>

<li class="on"><code>[X]</code> finish Att BLSTM paper.<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-21 Wed&gt;</span></span></li>
<li class="on"><code>[X]</code> 先完成“投资”这一类语料的收集。</li>
<li class="on"><code>[X]</code> 目标：按实体 实体 关系 语料内容的格式放入训练文件，以供模型训练。</li>
<li class="on"><code>[X]</code> 丰富语料的思路：通过word2vec 相似词找到“投资”的相似词，如设立，增资，入股，收购，并购，换股;再找以上6个词的相似词。</li>
</ul>
<p>
下表为投资这一大类所包含的相似关系。
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">设立</th>
<th scope="col" class="org-left">增资</th>
<th scope="col" class="org-left">入股</th>
<th scope="col" class="org-left">收购</th>
<th scope="col" class="org-left">并购</th>
<th scope="col" class="org-left">换股</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">成立</td>
<td class="org-left">受让</td>
<td class="org-left">现金出资</td>
<td class="org-left">要约收购</td>
<td class="org-left">海外并购</td>
<td class="org-left">转股</td>
</tr>

<tr>
<td class="org-left">发起设立</td>
<td class="org-left">扩股</td>
<td class="org-left">携手</td>
<td class="org-left">拟收购</td>
<td class="org-left">重组</td>
<td class="org-left">交换</td>
</tr>

<tr>
<td class="org-left">组建</td>
<td class="org-left">扩股</td>
<td class="org-left">间接持有</td>
<td class="org-left">并表</td>
<td class="org-left">整合</td>
<td class="org-left">配股</td>
</tr>

<tr>
<td class="org-left">新设</td>
<td class="org-left">占股</td>
<td class="org-left">所持</td>
<td class="org-left">过户</td>
<td class="org-left">兼并</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">出资</td>
<td class="org-left">转让给</td>
<td class="org-left">联手</td>
<td class="org-left">收购了</td>
<td class="org-left">业务整合</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">共同出资</td>
<td class="org-left">认缴</td>
<td class="org-left">正式成为</td>
<td class="org-left">资产收购</td>
<td class="org-left">借壳上市</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">全资</td>
<td class="org-left">定向增发</td>
<td class="org-left">转让给</td>
<td class="org-left">通过收购</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">参股</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">参股</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">入驻</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">创投</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="outline-container-orgc38a2fe" class="outline-5">
<h5 id="orgc38a2fe">实体和关系的联合抽取处理思想：<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-22 Thu&gt; </span></span> -</h5>
<div class="outline-text-5" id="text-orgc38a2fe">
</div>
<ul class="org-ul">
<li><a id="org00e9b51"></a>goal<br>
<div class="outline-text-6" id="text-org00e9b51">
<ol class="org-ol">
<li>利用NER<sub>IDCNN</sub><sub>CRF的实体识别得到语料里面的实体</sub>，现有模型支持人名，组织机构和位置。</li>
<li>RE<sub>BGRU</sub><sub>2ATT关系识别</sub>。</li>
<li class="off"><code>[&#xa0;]</code> Joint extraction of events and entities within a document context.</li>
</ol>
<p>
Conceptually the method can be applied to Chinese event extraction if you have a training corpus annotated with entities and events.
</p>

<p>
However, it would require significant changes on the code for feature generation. The current code makes use of the outputs of Stanford CoreNLP (English) and features extracted from English resources like WordNet, FrameNet, and NELL.
</p>
</div>
<ul class="org-ul">
<li><a id="org9117a0a"></a>Entity extraction<br>
<div class="outline-text-7" id="text-org9117a0a">
<p>
Extract company, signal, strategy from text documents.
</p>
</div>
</li>
<li><a id="org350b8c6"></a>Relation extraction<br>
<div class="outline-text-7" id="text-org350b8c6">
<p>
Extract relation between entities, which can create a knowledge network.
</p>
</div>
</li>
<li><a id="org7832680"></a>Event extraction<br></li>
</ul>
</li>


<li><a id="org331514c"></a><span class="done DONE">DONE</span> pseudo code<br>
<div class="outline-text-6" id="text-org331514c">
</div>
</li>
<li><a id="orga0e034d"></a>review<br>
<div class="outline-text-6" id="text-orga0e034d">
<ul class="org-ul">
<li>the limits of GRU, its memory performance without attention, find out the threshold.</li>
<li class="off"><code>[&#xa0;]</code> selecting GRU or LSTM Depends on length of input sentence.</li>
<li class="off"><code>[&#xa0;]</code> using existing Knowledge graph and collected  training data.</li>
<li class="off"><code>[&#xa0;]</code> use quantitative research, economic indicator formula, analyst report as training data.</li>
<li class="off"><code>[&#xa0;]</code> what's gold-standard entity information.</li>
</ul>
</div>
</li>
<li><a id="org2f71542"></a>study book DL for RE.<br>
<div class="outline-text-6" id="text-org2f71542">
<ul class="org-ul">
<li class="on"><code>[X]</code> GRU network, difference between LSTM. simpler.</li>
<li class="off"><code>[&#xa0;]</code> entity mention detection的过程和处理结构.</li>
</ul>
</div>
</li>


<li><a id="org2f206bc"></a>bugs:<br>
<ul class="org-ul">
<li><a id="org1a38580"></a>multiple white space in the entities.<br></li>
</ul>
</li>

<li><a id="org4171f82"></a><span class="done DONE">DONE</span> presentation<br>
<div class="outline-text-6" id="text-org4171f82">
<ul class="org-ul">
<li class="on"><code>[X]</code> RNN structure.</li>
<li class="on"><code>[X]</code> how to use RNN to extract entity.</li>
<li class="on"><code>[X]</code> GRU network, difference between LSTM.</li>
<li class="on"><code>[X]</code> Bi-directional LSTM.</li>
<li class="on"><code>[X]</code> build RNN tensorflow code.</li>
<li class="on"><code>[X]</code> pseudo code for GRU attention Relation Extraction.</li>
<li class="on"><code>[X]</code> 看deep learning for information extraction书relation extraction和event extraction.</li>
<li class="on"><code>[X]</code> 写summary。</li>
</ul>
</div>
</li>
</ul>
</div>

<div id="outline-container-orgb017331" class="outline-5">
<h5 id="orgb017331">TextRank</h5>
<div class="outline-text-5" id="text-orgb017331">
<ul class="org-ul">
<li class="on"><code>[X]</code> test text rank example.</li>
<li class="on"><code>[X]</code> paper - TextRank: Bringing Order into Texts.</li>
<li class="on"><code>[X]</code> plot graph.</li>
<li class="on"><code>[X]</code> pseudo code.</li>
</ul>
</div>
<ul class="org-ul">
<li><a id="org99c2ee7"></a><span class="done DONE">DONE</span> Keywords extraction<br>
<div class="outline-text-6" id="text-org99c2ee7">
<p>
Extract the keywords from a text document or news.
</p>
</div>
</li>

<li><a id="org2d9bf86"></a><span class="done DONE">DONE</span> implementation usecase on GS.<br>
<div class="outline-text-6" id="text-org2d9bf86">
<ul class="org-ul">
<li class="on"><code>[X]</code> create a research article node of pdf format.</li>
<li class="on"><code>[X]</code> convert the pdf to text.</li>
<li class="on"><code>[X]</code> get the highlight of the text.</li>
<li class="on"><code>[X]</code> change the number of highlights to a fraction of total words or certain number.</li>
<li class="on"><code>[X]</code> feed the highlight into next step.</li>
<li class="on"><code>[X]</code> create highlight nodes on GS.</li>
<li class="on"><code>[X]</code> list tool documents.</li>
</ul>
</div>
</li>
<li><a id="orgdf54ae7"></a><span class="done DONE">DONE</span> customize textrank to textrank4zh, change output keywords number.<br>
<div class="outline-text-6" id="text-orgdf54ae7">
</div>
</li>
<li><a id="org3a27585"></a>train model to recognize company, indicator, signal.<br></li>
<li><a id="org916ac51"></a>deep learning Named Entity Recognization(NER) model on GS.<br></li>
<li><a id="org12f740c"></a>问题<br>
<div class="outline-text-6" id="text-org12f740c">
<ul class="org-ul">
<li class="on"><code>[X]</code> 什么是textrank算法.</li>
</ul>
<p>
It's a <b>graph-based ranking</b> algorithm that deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> 它能解决什么问题。extractive summarization, keywords extraction.</li>
<li class="on"><code>[X]</code> 对语言类型（中英文）是否有要求：对语言没有要求。</li>
</ul>
</div>
</li>
<li><a id="org4a3ad39"></a><span class="done DONE">DONE</span> extract the structure of a document, represent as a graph<br>
<div class="outline-text-6" id="text-org4a3ad39">
<p>
<a href="https://www.iwencai.com/msgconsule/search?qs=pc_~soniu~info~all~resultpage~topsearchbox&amp;tid=report&amp;w=%E8%B4%B5%E5%B7%9E%E8%8C%85%E5%8F%B0">https://www.iwencai.com/msgconsule/search?qs=pc_~soniu~info~all~resultpage~topsearchbox&amp;tid=report&amp;w=%E8%B4%B5%E5%B7%9E%E8%8C%85%E5%8F%B0</a>
</p>


<div class="figure">
<p><img src="././img/knowledge_graph_report.png" alt="knowledge_graph_report.png">
</p>
</div>
</div>
</li>
</ul>
</div>
</div>
<div id="outline-container-orgfd7f3f5" class="outline-4">
<h4 id="orgfd7f3f5"><span class="todo TODO">TODO</span> information extraction system<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-04-04 Wed&gt;</span></span></h4>
<div class="outline-text-4" id="text-orgfd7f3f5">
</div>
<div id="outline-container-orgbe2fab7" class="outline-5">
<h5 id="orgbe2fab7">extract the knowledge from company and products.</h5>
<div class="outline-text-5" id="text-orgbe2fab7">

<div class="figure">
<p><img src="./img/knowledge_graph_fruit.png" alt="knowledge_graph_fruit.png">
</p>
</div>
</div>
</div>
<div id="outline-container-org9250e6a" class="outline-5">
<h5 id="org9250e6a">extract keywords from documents, create a knowledge graph.</h5>
</div>
<div id="outline-container-orgad5b09f" class="outline-5">
<h5 id="orgad5b09f"><span class="done DONE">DONE</span> opinion sentiment analysis. <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-09 Fri&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-16 Fri&gt;</span></span></h5>
<div class="outline-text-5" id="text-orgad5b09f">
</div>
<ul class="org-ul">
<li><a id="org932ff8d"></a><span class="done DONE">DONE</span> read<sub>RMDB</sub><sub>table</sub> -&gt; NLP<sub>sentiment</sub><sub>analysis</sub> -&gt; generate<sub>sentiment</sub><sub>signal</sub>.<br>
<div class="outline-text-6" id="text-org932ff8d">
<p>
<a href="https://nlp.stanford.edu/courses/cs224n/2011/reports/nccohen-aatreya-jameszjj.pdf">sentiment prediction</a>
</p>
<ul class="org-ul">
<li>Sentiment analysis 算法.</li>
</ul>
<p>
SVM, HMM, naive bayes, 最大熵, K-NN, Dictionary.
</p>
<ul class="org-ul">
<li>爬取Google news, 雪球， 虎嗅， 微信上所有300支股票的文档，再进行sentiment analysis, 结果再排序，选最好的5只。</li>
<li class="off"><code>[&#xa0;]</code> sentiment score做为单因子测试, upload sentiment data to hadoop and test factor in FS.</li>
<li class="on"><code>[X]</code> read paper <b>joint extraction of entities and relations</b>.</li>
<li class="off"><code>[&#xa0;]</code> read paper <b>Anomalies and Investor Sentiment</b>.</li>
<li class="on"><code>[X]</code> 情感分析指标的设计在GS上实现。</li>
<li class="on"><code>[X]</code> news, market-view articles sentiment analysis.</li>
<li class="on"><code>[X]</code> 发现2018-02-08, 情感指数0.54，2-9日出现大跌。</li>
</ul>
<p>
能否用这个指数来预警，今天可以扩大一下样本空间，看看上证在1%下跌的情况下前一日的情感指数值是如何变化。
</p>
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> search paper and books how to use sentiment analysis.</li>
</ul>
</div>
</li>
<li><a id="orge56a8a0"></a><span class="done DONE">DONE</span> train sentiment classification model.&#xa0;&#xa0;&#xa0;<span class="tag"><span class="review">review</span></span><br>
<div class="outline-text-6" id="text-orge56a8a0">
<p>
review:
</p>
<ol class="org-ol">
<li>分拆分类器的训练和分类。</li>
<li>训练好的模型存储在GID背后的路径。</li>
</ol>
</div>

<ul class="org-ul">
<li><a id="org74e82ad"></a>improvement:&#xa0;&#xa0;&#xa0;<span class="tag"><span class="improvement">improvement</span></span><br>
<div class="outline-text-7" id="text-org74e82ad">
<ol class="org-ol">
<li>应该早点把与sentiment classification action不太相关的步骤省略。</li>
<li>训练的步骤代码早点应该弄清楚：怎么调用，可以做几个分类？</li>
<li>不要hard code一些自己加进去的逻辑，如positive probability *2;</li>
</ol>
</div>
</li>
</ul>
</li>
</ul>
</div>

<div id="outline-container-org9087774" class="outline-5">
<h5 id="org9087774">information retrieval system</h5>
<div class="outline-text-5" id="text-org9087774">
</div>
<ul class="org-ul">
<li><a id="orgd8c79f6"></a>goal<br>
<ul class="org-ul">
<li><a id="org1e55ebd"></a>question and answering from a document<br>
<div class="outline-text-7" id="text-org1e55ebd">
<ul class="org-ul">
<li>what is tha data.</li>
<li>what is the algorithm.</li>
<li>what is the conclusion.</li>
</ul>
</div>
</li>
<li><a id="orgaa0df8f"></a>News summary<br>
<div class="outline-text-7" id="text-orgaa0df8f">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> classify 1 year of analyst research articles.</li>
<li class="on"><code>[X]</code> convert PDFs to text files.</li>
<li class="off"><code>[&#xa0;]</code> summerize articles</li>
</ul>
</div>
</li>
<li><a id="orgcf3bc00"></a>syntactic parsing<br></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org2b04a2d" class="outline-4">
<h4 id="org2b04a2d">Latent Dirichlet Allocation(LDA)</h4>
<div class="outline-text-4" id="text-org2b04a2d">
<p>
Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics.
</p>
</div>

<ul class="org-ul">
<li><a id="org774fa49"></a><span class="todo TODO">TODO</span> Word Embedding<br>
<div class="outline-text-6" id="text-org774fa49">
<p>
Recommend similar words.
</p>
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> change skill instance input as node.</li>
</ul>
</div>
</li>
<li><a id="org90593d7"></a><span class="done DONE">DONE</span> Sentiment analysis<br>
<div class="outline-text-6" id="text-org90593d7">
<p>
To create a sentiment index for a keyword, which can be provided as a indicator.
</p>
</div>
</li>

<li><a id="org0ea4c86"></a><span class="todo TODO">TODO</span> DeepDive<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-04-23 Mon&gt;</span></span><br>
<div class="outline-text-6" id="text-org0ea4c86">
</div>

<ul class="org-ul">
<li><a id="orgb59af07"></a>function<br>
<div class="outline-text-7" id="text-orgb59af07">
<p>
deepdive适合从unstructured data里面找出event, 而我们从scholar上爬下来的数据已经可以变成structured-data了，所以用实体消歧工具之后做entity linking即可。
</p>

<p>
deepdive的输入数据为unstructured-data，他需要利用Stanford CoreNLP从句子里面提取语义信息，包括词干，词性信息（主谓宾等），识别出来的实体，还有句子特征来帮助做信息抽取。
</p>
</div>
</li>
<li><a id="orgccca1e1"></a>feature<br></li>

<li><a id="orgc512513"></a>advantage<br></li>

<li><a id="org2114bb8"></a><span class="done DONE">DONE</span> start from structured data, entity linking these data.<br>
<div class="outline-text-7" id="text-org2114bb8">
</div>
</li>
</ul>
</li>
</ul>
</div>
<div id="outline-container-orgc735254" class="outline-4">
<h4 id="orgc735254"><span class="done DONE">DONE</span> Word Embedding(Word2Vec):<span class="timestamp-wrapper"><span class="timestamp">&lt;2017-12-01 Fri&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2017-12-31 Sun&gt;</span></span></h4>
<div class="outline-text-4" id="text-orgc735254">
</div>
<div id="outline-container-org4ecf97c" class="outline-5">
<h5 id="org4ecf97c">Goal/use case</h5>
<div class="outline-text-5" id="text-org4ecf97c">
<ul class="org-ul">
<li>use such word2vec to find similar keywords.</li>
</ul>
</div>
</div>
<div id="outline-container-org192e61e" class="outline-5">
<h5 id="org192e61e">jobs: 数据收集， 清洗</h5>
<div class="outline-text-5" id="text-org192e61e">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> train analyst report and save model on hdfs, load this model as a j node.</li>
<li class="off"><code>[&#xa0;]</code> gs similar words function test use analyst report.</li>
<li class="on"><code>[X]</code> upload all vocabulary in word2vec model to Neo4j.</li>
<li class="on"><code>[X]</code> create a function: word<sub>rec</sub>(model, keywords, topn)</li>
<li class="off"><code>[&#xa0;]</code> manually add categories and page links in sql file.</li>
<li class="on"><code>[X]</code> return word embedding model to next step in GS.</li>
<li class="on"><code>[X]</code> word embedding的设计文档修改.</li>
<li class="on"><code>[X]</code> pack pages into a corpus file.</li>
<li class="off"><code>[&#xa0;]</code> compare cutting on paragraph and document.</li>
<li class="off"><code>[&#xa0;]</code> train few files to see if there's repeat training on word2vec.</li>
<li class="off"><code>[&#xa0;]</code> extract data from financial documents — usually PDFs — in an automated way, and to produce “better-than-human” analyses. extract data from tables and text.</li>
<li class="off"><code>[&#xa0;]</code> train function names based on wiki pages on functions, models, and python/matlab/sas/cpp-reference manuals, function names and function descriptions, excel formula, VBA, VB, guass, whatever software which has a function dictionary and manual.</li>
<li class="off"><code>[&#xa0;]</code> retrieve pages title and id under categories from mysql.</li>
<li class="off"><code>[&#xa0;]</code> LSA or LDA analysis on unstructured text, which will give a clustering of words on every topic.</li>
<li class="off"><code>[&#xa0;]</code> visualize vocabulary embedding using t-SNE which project embedding vectors into 2-D surface from an proper perspective using tensorboard locally which can ignore uploading to projector online.</li>
<li class="off"><code>[&#xa0;]</code> create LSTM networks on xarray data.</li>
<li class="on"><code>[X]</code> create test program to run word embedding, to visualize output.</li>
<li class="off"><code>[&#xa0;]</code> <b>What is fueling heavy investment in machine learning in the financial industry and how does it fit into customers’ workflows?</b>
A lot of our customers’ workflows are being automated, entirely or partially. What they’re doing today is more on the cognitive side: strategy and portfolio selection, formulating the investment theses, etc. People are trying to solve many, many problems in finance using these methods, because they allow for the building of more sophisticated intelligence into trading and client facing workflows. These methods can improve efficiency, or, crucially, allow us to approach problems which heretofore were intractable – due to complicated interactions in the data, complexity of the problem, availability of data or computational resources, and so on.</li>
<li class="on"><code>[X]</code> provide xarray data to Zhou.</li>
<li class="on"><code>[X]</code> provide Sun Chinese wiki.</li>
<li class="on"><code>[X]</code> network Bloomberg about tensorflow.</li>
<li class="on"><code>[X]</code> retrieving speed test from mongodb.</li>
<li class="on"><code>[X]</code> test case on finance domain word embedding prediction.</li>
<li class="on"><code>[X]</code> dumping wiki pages to mongodb.</li>
<li class="on"><code>[X]</code> testing GPU server.</li>
<li class="on"><code>[X]</code> configuring deep learning hardware, operation system, software.</li>
<li class="on"><code>[X]</code> test sets simularity, A-B=C-D?, A+B=?</li>
<li class="on"><code>[X]</code> incremental training finance pages based via online training.
online training can not continue missing frequency in pretrained google binary file.</li>
<li class="off"><code>[&#xa0;]</code> cut/training Chinese osets words into files.</li>
<li class="off"><code>[&#xa0;]</code> compare similarity between category and end-to-node oset element.</li>
<li class="off"><code>[&#xa0;]</code> compare the results from GS searching engine and word embedding.</li>
<li class="off"><code>[&#xa0;]</code> import xml pages to elasticsearch.</li>
<li class="on"><code>[X]</code> clustering categories by word embedding, osets, idea.
To calculate the similarity matrix between all 160706 vocabulary in RAM, 160706 *160706 *4(bytes)/1024(bytes)/1014(bytes)=99491MB will be needed.</li>
<li class="on"><code>[X]</code> use <a href="http://www.cis.lmu.de/pub/phraseEmbedding.txt.bz2">phrase embedding</a> as test.
better phrasing results.</li>
<li class="on"><code>[X]</code> take a look at cite space iii.</li>
<li class="on"><code>[X]</code> test word2vec model from finance.</li>
<li class="on"><code>[X]</code> cut paragraph to short sentences, then phrase.</li>
<li class="on"><code>[X]</code> phrase text8</li>
<li class="on"><code>[X]</code> train phrasing sentences word2vec model.</li>
<li class="on"><code>[X]</code> phrase detection with google pretrained vectors.</li>
<li class="on"><code>[X]</code> find available library to extract wiki content.</li>
<li class="on"><code>[X]</code> find all page titles from level 5 finance sub-categories.</li>
<li class="on"><code>[X]</code> extract page section from wiki xml file.</li>
<li class="off"><code>[&#xa0;]</code> parse Chinese wiki, remove stopwords.</li>
<li class="on"><code>[X]</code> model wiki token corpus.</li>
<li class="off"><code>[&#xa0;]</code> <a href="https://github.com/ryankiros/skip-thoughts">skip-thought</a>.</li>
<li class="on"><code>[X]</code> find corporate finance/mba questions corpos.</li>
<li class="off"><code>[&#xa0;]</code> read A primer on Neural Network Models.</li>
<li class="on"><code>[X]</code> tensorflow structure.</li>
<li class="on"><code>[X]</code> train word2vec model.</li>
<li class="on"><code>[X]</code> test finding similar words from Wiki corpus.</li>
<li class="on"><code>[X]</code> download wiki xml file.</li>
<li class="on"><code>[X]</code> transfer wiki xml file to text format.</li>
<li class="on"><code>[X]</code> load pre-trained vector matrix, predict the context using a word based on the Skip-Gram model.</li>
<li class="on"><code>[X]</code> overview of word2vec, why does it work.</li>
<li class="off"><code>[&#xa0;]</code> video explained by Xin Rong.</li>
<li class="off"><code>[&#xa0;]</code> forward propagation vs backward propagation, CNN explained by Andrew Ng.</li>
<li class="on"><code>[X]</code> paper word2vec Parameter Learning Explained.</li>
<li class="on"><code>[X]</code> understand Tensorflow Word2Vec example.</li>
<li class="on"><code>[X]</code> build a backward propagation network.</li>
<li class="off"><code>[&#xa0;]</code> fi or function def from output of wants whose idea word2vec is close to target want's idea.</li>
</ul>
<p>
建一个想法，根据这个想法找到匹配的FI, or FD. 例如，建一个optimize需求，自动推荐black litterman model, or markowitz mean/variance model.
</p>
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> fi and its function def whose word2vec is close to word2vec of function instances of current function def to be built.</li>
</ul>
<p>
当前FI,查找相关的下一步FI.
</p>
</div>
</div>
</div>

<div id="outline-container-org4e88814" class="outline-4">
<h4 id="org4e88814"><span class="done DONE">DONE</span> Chinese wiki model. <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-01-01 Mon&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-01-12 Fri&gt;</span></span></h4>
<div class="outline-text-4" id="text-org4e88814">
</div>
<div id="outline-container-org372eca7" class="outline-5">
<h5 id="org372eca7">jobs: 训练中文维基数据，嵌入GS</h5>
<div class="outline-text-5" id="text-org372eca7">
<ul class="org-ul">
<li>choose model using most related model, use wiki category relation similarity to choose model, train specific field category model. get the related category tree, use regular expression to get responding categories from the wiki xml file.</li>
<li class="on"><code>[X]</code> train financial fields model(58+ categories).</li>
<li class="on"><code>[X]</code> use similarity distance to find the nearest category of target words.</li>
<li class="on"><code>[X]</code> similarity test on specific model.</li>
<li class="on"><code>[X]</code> add all pages title to jieba dict.</li>
<li class="on"><code>[X]</code> 中文短语处理，当短语不存在词汇库中时，拆开成词输入到模型。</li>
<li class="on"><code>[X]</code> <p>
preprocessing workflow.
英文text preprocessing需要的注意一些点，及应提供的选择
</p>
<ol class="org-ol">
<li>cut段落或文章</li>
<li>phrase是否进行转换</li>
<li>停词(a, the, of, that, this, he, I&#x2026;)是否保留</li>
<li>数字是否转为英文单词, 中间有数字的单词是否保留(th8)</li>
<li>提取词干（时态转换，单复数单词转换）</li>
<li>标点（撇号'，所有格,缩写如don’t），符号（%,#,&amp;,?,@,\,/,",是否保留）</li>
<li>大小写转换（句首大写转小写，保留全部大写词，专有名词首字母大写保留）</li>
</ol>

<p>
中文分词（主要利用结巴分词）
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> 1. cut段落或文章</li>
<li class="on"><code>[X]</code> 2. 去停词</li>
<li class="on"><code>[X]</code> 去标点符号</li>
<li class="on"><code>[X]</code> 去数字</li>
</ul></li>
<li class="off"><code>[&#xa0;]</code> word2vec fast text comparison.</li>
<li class="on"><code>[X]</code> compare the training results with or without stopwords.</li>
<li class="on"><code>[X]</code> demo code.</li>
<li class="on"><code>[X]</code> visualize &amp; compare results.</li>
<li class="on"><code>[X]</code> create index for zhwiki.</li>
<li class="on"><code>[X]</code> test model.</li>
<li class="on"><code>[X]</code> assign wiki pages extraction task.</li>
<li class="on"><code>[X]</code> insert Chinese wiki to mongo, transform traditional Chinese to simple Chinese.</li>
<li class="on"><code>[X]</code> get rid of the stopwords.</li>
<li class="on"><code>[X]</code> retrie Chinese financial wiki pages from mongo and train.</li>
<li class="off"><code>[&#xa0;]</code> fix zhwiki to mongodb words count.
:wait:</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org6ac56bc" class="outline-4">
<h4 id="org6ac56bc"><span class="done DONE">DONE</span> Building the Wikipedia Knowledge Graph in Neo4j <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-01-13 Sat&gt; </span></span> - <span class="timestamp-wrapper"><span class="timestamp">&lt;2018-03-09 Fri&gt;</span></span></h4>
<div class="outline-text-4" id="text-org6ac56bc">
<p>
<a href="file:///home/weiwu/website/leolle.github.io/CS/MachineLearning/NaturalLanguageProcessing.html">NLP</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> wiki SQL database links graph.</li>
<li class="on"><code>[X]</code> pulling wiki knowledge categories(id), pages(id) and relations to local csv, sql file.</li>
</ul>
</div>
<div id="outline-container-org58fec4f" class="outline-5">
<h5 id="org58fec4f"><span class="done DONE">DONE</span> Data dumps/Import -&gt; create nodes</h5>
<div class="outline-text-5" id="text-org58fec4f">
<ul class="org-ul">
<li>methods</li>
</ul>
<p>
<a href="https://meta.wikimedia.org/wiki/Data_dumps">https://meta.wikimedia.org/wiki/Data_dumps</a>
</p>

<p>
<a href="https://meta.wikimedia.org/wiki/Data_dumps/Import_examples">https://meta.wikimedia.org/wiki/Data_dumps/Import_examples</a>
</p>

<p>
<a href="https://phabricator.wikimedia.org/source/operations-dumps-import-tools/browse/master/xmlfileutils/">https://phabricator.wikimedia.org/source/operations-dumps-import-tools/browse/master/xmlfileutils/</a>
</p>
<ul class="org-ul">
<li>tools</li>
</ul>
<p>
<a href="http://wikipapers.referata.com/wiki/List_of_visualization_tools">http://wikipapers.referata.com/wiki/List_of_visualization_tools</a>
</p>

<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> Import into an empty wiki of el wiktionary on Linux with MySQL, or Neo4j</li>
<li class="off"><code>[&#xa0;]</code> create special wiki reference edge between read only text nodes</li>
<li class="on"><code>[X]</code> watch the youtube video</li>
</ul>
<p>
<a href="https://www.youtube.com/watch?v=o6wueyweC34%20">https://www.youtube.com/watch?v=o6wueyweC34%20</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> read Neo4j document</li>
</ul>
<p>
<a href="http://guides.neo4j.com/wiki">http://guides.neo4j.com/wiki</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> try Neo4j sandbox</li>
</ul>
<p>
<a href="https://neo4j.com/sandbox-v2/">https://neo4j.com/sandbox-v2/</a>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> create Neo4j docker.</li>
</ul>
</div>
</div>
<div id="outline-container-org44084c1" class="outline-5">
<h5 id="org44084c1"><span class="done DONE">DONE</span> create wiki knowledge graph -&gt; create edges</h5>
<div class="outline-text-5" id="text-org44084c1">
<ul class="org-ul">
<li class="on"><code>[X]</code> extract gid from get skill to graph.</li>
<li class="on"><code>[X]</code> importing wiki categories and page edge relation to Neo4j.</li>
<li class="on"><code>[X]</code> 上传完备份我再建边.</li>
<li class="on"><code>[X]</code> 加一个loop detection算法，现在只做了direct cycle detection algorithm.
<ul class="org-ul">
<li class="on"><code>[X]</code> use networkx to detect loop.</li>
<li class="on"><code>[X]</code> it's too hard to detect cycles in the whole graph. Starting in a small categories.</li>
<li class="on"><code>[X]</code> don't add direct loop edges to a graph, find<sub>cycles</sub> will only show such direct loop. save this graph.</li>
<li class="on"><code>[X]</code> remove direct cycle and full cycle at a node completely.</li>
</ul></li>
<li class="on"><code>[X]</code> skill GID generating in Python.</li>
<li class="on"><code>[X]</code> 把节点上传. wiki 上传了1040229 page, 381475 categories.</li>
<li class="on"><code>[X]</code> train word2vec model based on GID.</li>
<li class="on"><code>[X]</code> import edge, loop detecting for linking categories nodes.</li>
<li class="on"><code>[X]</code> fetching pages binary content via GID.</li>
<li class="on"><code>[X]</code> test response GID, same with GID saved on Chrome.</li>
<li class="on"><code>[X]</code> test fetching binary text with GID.</li>
<li class="on"><code>[X]</code> extract page to neo4j from xml file.</li>
</ul>
<p>
businessID.domain = <a href="https://zh.wikipedia.org/wiki/">https://zh.wikipedia.org/wiki/</a>:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
node.url = encoded<sub>url</sub>
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> import category to neo4j from sql file.</li>
</ul>
<p>
businessID.domain = <a href="https://zh.wikipedia.org/wiki/Category">https://zh.wikipedia.org/wiki/Category</a>:
businessID.pk = urlencode(traditional Chinese title).
title = simple Chinese title
node.names.chinese = simple Chinese title
</p>
<ul class="org-ul">
<li class="on"><code>[X]</code> double check GID with Shenbing after importing a small set of page.</li>
<li class="on"><code>[X]</code> import page from mongo to neo4j.</li>
<li class="on"><code>[X]</code> backup neo4j after importing categories and page.</li>
<li class="on"><code>[X]</code> delete edges.</li>
<li class="on"><code>[X]</code> importing wiki categories nodes and page nodes to Neo4j.</li>
<li class="on"><code>[X]</code> test importing wiki categories nodes.</li>
<li class="on"><code>[X]</code> skill<sub>2</sub><sub>graph</sub></li>
</ul>
<p>
<code>C-M-r</code> in gs, create 查路径, drag GID: 81F49335AC9C4D84A5F27F7A02AAABBA into the input box, input Parent GID in the parent box.
</p>
</div>
<ul class="org-ul">
<li><a id="org00d0e4d"></a>Thomson Reuters Knowledge graph perim<br>
<div class="outline-text-6" id="text-org00d0e4d">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> read how to use the RFM dataset.</li>
</ul>
</div>
</li>
<li><a id="org11f7de1"></a>relation extraction from training data<br>
<div class="outline-text-6" id="text-org11f7de1">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> search paper and public code.</li>
<li class="on"><code>[X]</code> Stanford NLP relation extraction video.</li>
</ul>
</div>
</li>
</ul>
</div>
<div id="outline-container-orgf491fdd" class="outline-5">
<h5 id="orgf491fdd"><span class="done DONE">DONE</span> manual import unsaved categories and edges into Neo4j.</h5>
<div class="outline-text-5" id="text-orgf491fdd">
<ul class="org-ul">
<li class="on"><code>[X]</code> find unsaved categories under 金融 category.</li>
<li class="on"><code>[X]</code> save those to a sql file.</li>
<li class="on"><code>[X]</code> upload sql file and edge.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org76cd6bc" class="outline-4">
<h4 id="org76cd6bc"><span class="todo TODO">TODO</span> NLP information system workflow<span class="timestamp-wrapper"><span class="timestamp">&lt;2018-04-11 Wed&gt; </span></span> -</h4>
<div class="outline-text-4" id="text-org76cd6bc">
</div>
<div id="outline-container-org4b3a171" class="outline-5">
<h5 id="org4b3a171">Overview:</h5>
<div class="outline-text-5" id="text-org4b3a171">
</div>
<ul class="org-ul">
<li><a id="org46591c7"></a>GS can easily help users increase working efficiency on:<br>
<div class="outline-text-6" id="text-org46591c7">
<ul class="org-ul">
<li>Idea search on data/document</li>
<li>Model building</li>
</ul>
</div>
</li>
<li><a id="org5ed60eb"></a>NLP information system integrated into document search.<br>
<ul class="org-ul">
<li><a id="orgefdd336"></a>Increase working efficiency features:<br>
<ul class="org-ul">
<li><a id="orgf30d6d6"></a>Interaction between GS current task of workflow and web browser page.<br>
<div class="outline-text-8" id="text-orgf30d6d6">
<p>
Interaction is based on:
</p>
<ul class="org-ul">
<li>Context information of workflow, current main task</li>
<li>Web browser page content</li>
</ul>
</div>
</li>
<li><a id="org351256d"></a>Recommend documents based on user profile.<br></li>
<li><a id="orgf3e7d83"></a>Find user preferred data, indicator, factor, strategies, models.<br></li>
<li><a id="org3bcf146"></a>Find connected data/company/node from knowledge graph.<br></li>
<li><a id="orgb8d5059"></a>User defined policy can be automatically triggered on schedule or by action.<br></li>
<li><a id="org69d77a4"></a>Users in a group can collaboratively finish the same goal.<br></li>
</ul>
</li>
<li><a id="org3f25544"></a>Use case:<br>
<ul class="org-ul">
<li><a id="orgb5a1834"></a>Switch current main task according to browsing website page.<br></li>
<li><a id="org8d8dce2"></a>GS automatically highlight entities on web browser page based on current main task.<br></li>
<li><a id="orgb93bce3"></a>Store labeled text for training corpus.<br></li>
<li><a id="orgdd03bb2"></a>Provide user preferred data, indicator, factor, strategies, models trained from wiki categories and page.<br></li>
</ul>
</li>
<li><a id="org1af2f3d"></a>Principle of use case i:<br>
<div class="outline-text-7" id="text-org1af2f3d">
<ul class="org-ul">
<li>Use text summarization tool can extract theme of page content.</li>
<li>Use the extracted theme to trigger a current main task change.</li>
</ul>
</div>
</li>
<li><a id="orgd7ab4ec"></a>Principle of use case ii:<br>
<div class="outline-text-7" id="text-orgd7ab4ec">
<ul class="org-ul">
<li>Embedding all node text from current main task together with text from page content.</li>
<li>Find similar sentences from current main task’s vector.</li>
</ul>
</div>
</li>

<li><a id="org5c6f98d"></a>Define policy, state variable, reward:<br>
<div class="outline-text-7" id="text-org5c6f98d">
<p>
Policy and state variable:
</p>
</div>

<ul class="org-ul">
<li><a id="org067dfd1"></a>Reward:<br>
<div class="outline-text-8" id="text-org067dfd1">
<ul class="org-ul">
<li>If agent finds target data, indicator, factor, strategies, models and continue his workflow.</li>
<li>If user shares this document in the group, or agent shares it to other users.</li>
<li>If user labeled additional text from the document.</li>
<li>If user check this document again.</li>
</ul>
</div>
</li>
</ul>
</li>
<li><a id="org355e911"></a>technique/library description:<br>
<div class="outline-text-7" id="text-org355e911">
<p>
<a href="https://docs.google.com/spreadsheets/d/1j10vRQhwOWBLtYsJjsbgXqI5XyCDagSF708pano1JEc/edit#gid=0">https://docs.google.com/spreadsheets/d/1j10vRQhwOWBLtYsJjsbgXqI5XyCDagSF708pano1JEc/edit#gid=0</a>
</p>
</div>
</li>
</ul>
</li>
</ul>
</div>
<div id="outline-container-orgbbee6ea" class="outline-5">
<h5 id="orgbbee6ea">use cases:</h5>
<div class="outline-text-5" id="text-orgbbee6ea">
</div>
<ul class="org-ul">
<li><a id="org15a44e6"></a><span class="done DONE">DONE</span> Use case 1:<br>
<div class="outline-text-6" id="text-org15a44e6">
<p>
Switch current main task according to browsing website page:
</p>
</div>
<ul class="org-ul">
<li><a id="org3a06900"></a>Input: Example document:<br>
<div class="outline-text-7" id="text-org3a06900">
<p>
20170122-长江证券-长江证券金融工程：基于网络的动量选股策略
</p>
</div>
</li>
<li><a id="orga522bd3"></a>URL:<br>
<div class="outline-text-7" id="text-orga522bd3">
<p>
<a href="http://q.gftchina.com:13567/DocUIHTML/pdf/web/viewer.html?file=/vqservice/vq/docs/BB362E31FF4D41C234A804E3653030B1">http://q.gftchina.com:13567/DocUIHTML/pdf/web/viewer.html?file=/vqservice/vq/docs/BB362E31FF4D41C234A804E3653030B1</a>
</p>
</div>
</li>
<li><a id="org60be31a"></a>Available tools:<br>
<div class="outline-text-7" id="text-org60be31a">
<ul class="org-ul">
<li>TextRank summarization</li>
<li>Tokenization</li>
<li>Word Embedding</li>
</ul>
</div>
</li>
<li><a id="org9cfcbca"></a>Description:<br>
<div class="outline-text-7" id="text-org9cfcbca">
<ul class="org-ul">
<li>extract document theme from opened website.</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #F92672;">def</span> <span style="color: #A6E22E;">extract_theme</span><span style="color: #AE81FF;">(</span>text<span style="color: #AE81FF;">)</span>:
    <span style="color: #FD971F;">tr4w</span> = TextRank4Keyword<span style="color: #AE81FF;">()</span>

    tr4w.analyze<span style="color: #AE81FF;">(</span>
        text=text, lower=<span style="color: #AE81FF;">False</span>,
        window=<span style="color: #AE81FF;">2</span><span style="color: #AE81FF;">)</span>
    <span style="color: #75715E;"># </span><span style="color: #75715E;">&#33719;&#21462;&#20851;&#38190;&#30701;&#35821;&#12290;&#33719;&#21462; keywords_num &#20010;&#20851;&#38190;&#35789;&#26500;&#36896;&#30340;&#21487;&#33021;&#20986;&#29616;&#30340;&#30701;&#35821;&#65292;&#35201;&#27714;&#36825;&#20010;&#30701;&#35821;&#22312;&#21407;&#25991;&#26412;&#20013;&#33267;&#23569;&#20986;&#29616;&#30340;&#27425;&#25968;&#20026;min_occur_num&#12290;</span>
    <span style="color: #FD971F;">theme_result</span> = tr4w.get_keyphrases<span style="color: #AE81FF;">(</span>keywords_num=<span style="color: #AE81FF;">20</span>, min_occur_num=<span style="color: #AE81FF;">2</span><span style="color: #AE81FF;">)</span>
    <span style="color: #F92672;">return</span> theme_result
</pre>
</div>

<pre class="example">
Output:
主题：
模型指标
股票动量
策略模型
股价股票
</pre>
</div>
</li>
<li><a id="org5324678"></a>Store theme to state variable.<br></li>
<li><a id="org21d8090"></a>Trigger workflow policy change task action.<br>
<div class="outline-text-7" id="text-org21d8090">
<ol class="org-ol">
<li>说明library, algorithm 的使用场景。</li>
<li>比较bag of words与textrank的使用效果。</li>
<li>前期花了很多时间在数据准备的工具制作上，给后期使用数据提供了方便，接下来还需要把一些算法具体的细节搞明白，如跟其它算法相比有什么不同，有什么优势，效果怎么样等。</li>
<li>列出还没完全搞明白算法。</li>
</ol>
</div>
</li>
</ul>
</li>

<li><a id="org223f55c"></a><span class="done DONE">DONE</span> Use case 2:<br>
<div class="outline-text-6" id="text-org223f55c">
<p>
highlight entities/sentences according to GS current main task.
</p>
</div>
<ul class="org-ul">
<li><a id="orgcbf2157"></a>Input: current task nodes text and web page content<br></li>
<li><a id="org841fcc8"></a>example task: 查找动量指标, equivalent want.<br></li>
<li><a id="orgcb576b7"></a>Available tools/techniques:<br>
<div class="outline-text-7" id="text-orgcb576b7">
<ul class="org-ul">
<li>Word embedding</li>
<li>LDA</li>
<li>Named Entity Recognition</li>
</ul>
</div>
</li>
<li><a id="org6445956"></a>Description<br>
<div class="outline-text-7" id="text-org6445956">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> Embedding all nodes text.</li>
<li class="off"><code>[&#xa0;]</code> Find nearest vectors between entities/sentences and current node.</li>
<li class="off"><code>[&#xa0;]</code> Find most similar words in the web browser page from node text.</li>
</ul>
<pre class="example">
Output:
model.most_similar(u'指标')
'市值', '模型', '节点', '网络', '专题报告', 'DEA', '股价', '平均线', 'MF', '策略', '声明', '目录', '速度', '流量'
model.most_similar(u'动量')
'距离', '平均线', 'NMF', '收盘价', '指数', '速度感', '节点', '概率', '资金', '股价', '行业', '散户', '策略', '指标', '速率', 'DEA'
model.most_similar(u'策略')
'指标', '节点', '模型', '动力学', '方法', '散户', '动量', '速率', '速度', '流量', '股价', '资金', '速度感'
model.most_similar(u'模型')
'指标', '散户', '网络', '策略', '数值', '最高价', '目录', '组分', '联系人', '专题报告', '基础', '股价', '历史数据', '股票走势'
</pre>
</div>
</li>
<li><a id="orga0ad83c"></a>context: 选择了节点，打开了相关的网页<br>
<div class="outline-text-7" id="text-orga0ad83c">
<p>
goal:
</p>
<ol class="org-ol">
<li>手动或者自动高亮similar words</li>
<li>根据网页内容找出可能的task下一步动作，例如是制作某个因子，建立某个模型等。</li>
<li>通过node的信息，如category, Oset, event，relation，找出网页对应的entity。</li>
</ol>
</div>
</li>
</ul>
</li>
<li><a id="org5acc741"></a><span class="todo TODO">TODO</span> Use case 4:<br>
<ul class="org-ul">
<li><a id="orgec889b4"></a>Goal: 通过node的信息，如category, Oset, event，relation，找出网页对应的entity. 根据当前收集到的信息，自动高亮网页中关于基本面的关键字.<br>
<div class="outline-text-7" id="text-orgec889b4">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> collect Oset dictionary
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> use wikipedia categories and page hierarchy tree to find similar words.</li>
</ul></li>
<li class="off"><code>[&#xa0;]</code> collect relationship dictionary</li>
</ul>
</div>
</li>

<li><a id="orgff4fea8"></a>input(node, task, url):<br>
<div class="outline-text-7" id="text-orgff4fea8">
<p>
Task: 当前任务的前一步是在构造基本面因子任务。
Selected node: 福耀玻璃(i)
Url: 查看福耀玻璃的网页信息，<a href="https://xueqiu.com/3075122481/105256619">https://xueqiu.com/3075122481/105256619</a>
</p>
</div>
</li>
<li><a id="org30729e1"></a>Tools:<br>
<div class="outline-text-7" id="text-org30729e1">
<p>
OSet scheme.
</p>
</div>
</li>
<li><a id="org097fa8e"></a>Output:<br>
<div class="outline-text-7" id="text-org097fa8e">
<p>
营业收入，利润，净利润，毛利率，每股收益，财务指标，市值，市盈率，市净率，PE，市场占有率，增长率等。
</p>
</div>
</li>
</ul>
</li>
<li><a id="org2533625"></a>Use case 5:<br>
<ul class="org-ul">
<li><a id="orgbe7221b"></a>Goal: automatically collect relation(based on existed scheme like 投资，兼并) in the website content, recommend keyword pair.<br></li>
<li><a id="orgd25c050"></a>input(url, task, node)<br>
<div class="outline-text-7" id="text-orgd25c050">
<p>
Url: 查看福耀玻璃的网页信息，<a href="https://xueqiu.com/3075122481/105256619">https://xueqiu.com/3075122481/105256619</a>
Task: 查找数据
Selected node: 福耀玻璃(i)
</p>
</div>
</li>
<li><a id="org715fdd4"></a>Output:<br>
<div class="outline-text-7" id="text-org715fdd4">
<p>
加拿大皇家银行增持
</p>
</div>
</li>
</ul>
</li>
<li><a id="org7d54402"></a>Use case 6:<br>
<ul class="org-ul">
<li><a id="org3d05196"></a>Goal: automatically recommend next step task/model use website extracted relation.<br></li>
<li><a id="org8de0051"></a>input(url, task, node, relation)<br>
<div class="outline-text-7" id="text-org8de0051">
<p>
Url: 查看福耀玻璃的网页信息，<a href="https://xueqiu.com/3075122481/105256619">https://xueqiu.com/3075122481/105256619</a>
Task: 查找数据
Selected node: 福耀玻璃(i)
Relation: 加拿大皇家银行增持
</p>
</div>
</li>
<li><a id="org053739e"></a>Output:<br>
<div class="outline-text-7" id="text-org053739e">
<p>
Similar stocks(under the same industry)  increase holding event happened.
</p>
</div>
</li>
</ul>
</li>
</ul>
</div>
<div id="outline-container-org8c7021e" class="outline-5">
<h5 id="org8c7021e">news recommendation with RL</h5>
<div class="outline-text-5" id="text-org8c7021e">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> web scraping case study.</li>
</ul>
<p>
<a href="https://www.octoparse.com/tutorial/web-scraping-case-study-scraping-articles-from-news24/">https://www.octoparse.com/tutorial/web-scraping-case-study-scraping-articles-from-news24/</a>
</p>

<ul class="org-ul">
<li class="trans"><code>[-]</code> list available web crawling scripts.
<ul class="org-ul">
<li class="on"><code>[X]</code> google scholar: <a href="https://github.com/ckreibich/scholar.py">https://github.com/ckreibich/scholar.py</a>
<ul class="org-ul">
<li>ip轮询</li>
<li>user agent随机</li>
<li>domain随机</li>
<li>休眠</li>
<li>cookies</li>
</ul></li>
<li class="off"><code>[&#xa0;]</code> google search</li>
<li class="off"><code>[&#xa0;]</code> google news</li>
</ul></li>
<li class="off"><code>[&#xa0;]</code> focused crawler with reinforcement learning.</li>
</ul>
</div>
<ul class="org-ul">
<li><a id="orge7e085c"></a>detecting and visualizing emerging technology trends.<br>
<ul class="org-ul">
<li><a id="orgc0814eb"></a>steps breakdown to create such workflow.<br></li>
<li><a id="org45d36a9"></a>主题词网络分析<br>
<div class="outline-text-7" id="text-org45d36a9">
<ul class="org-ul">
<li class="on"><code>[X]</code> propose use case details.</li>
<li class="on"><code>[X]</code> list available web crawler.</li>
<li class="on"><code>[X]</code> modify current web crawler.</li>
<li class="off"><code>[&#xa0;]</code> unit test crawler.</li>
<li class="off"><code>[&#xa0;]</code> create crawling function.</li>
</ul>
</div>
</li>
<li><a id="org9e4d6e6"></a>主题词时间序列分析<br></li>
<li><a id="orgfcdb371"></a>主题词s curve分析<br></li>
<li><a id="org8ab73da"></a>extract title, author, date from pdf.<br></li>
</ul>
</li>
<li><a id="org9170819"></a>Search answer on the website based on the keywords from the documents combined with the question.<br>
<div class="outline-text-6" id="text-org9170819">
<p>
Use attention/theme to give hints of the conversation.
</p>
</div>
</li>
<li><a id="org120ace1"></a>entity linking<br></li>
</ul>
</div>
</div>
<div id="outline-container-org8c8aaa9" class="outline-4">
<h4 id="org8c8aaa9">scholar paper download:</h4>
<div class="outline-text-4" id="text-org8c8aaa9">
</div>
<div id="outline-container-orgd71101e" class="outline-5">
<h5 id="orgd71101e">Download metadata with the article title:</h5>
<div class="outline-text-5" id="text-orgd71101e">
<ol class="org-ol">
<li class="on"><code>[X]</code> Find meta data with article title from crossref.org API.</li>

<li class="on"><code>[X]</code> Download specific articles directly(article url) or via sci-hub based on DOI.</li>
</ol>
</div>
</div>

<div id="outline-container-org2a7b5f6" class="outline-5">
<h5 id="org2a7b5f6"><span class="done DONE">DONE</span> Download a  article.</h5>
<div class="outline-text-5" id="text-org2a7b5f6">
<ul class="org-ul">
<li class="on"><code>[X]</code> Search for articles on Google Scholar and download them.</li>
<li class="on"><code>[X]</code> Download pdf directly if possible.</li>
<li class="on"><code>[X]</code> Download specific articles directly(article url) or via sci-hub based on url.
The actual pdf source url behind sci-hub is embedded in a iframe with the link looks something like</li>

<li class="on"><code>[X]</code> Get DOI from sci-hub content if possible, else search crossref with title, then search metadata for the article on crossref.</li>
<li class="off"><code>[&#xa0;]</code> or use google search.</li>
</ul>
</div>

<ul class="org-ul">
<li><a id="orgb0958a5"></a><span class="done DONE">DONE</span> run code on GS.<br>
<div class="outline-text-6" id="text-orgb0958a5">
</div>
</li>

<li><a id="org7bb6df5"></a><span class="todo TODO">TODO</span> upload article and metadata on GS.<br></li>

<li><a id="orgd65a064"></a>entity linking for author.<br></li>

<li><a id="org2ff08b3"></a><span class="todo TODO">TODO</span> extract table and figure from pdf.<br>
<div class="outline-text-6" id="text-org2ff08b3">
<p>
use pdfminer.six dumppdf.py
</p>
</div>
</li>
<li><a id="org4faa28b"></a>rename an uploaded pdf.<br></li>

<li><a id="orga3d3475"></a>Usage:<br>
<div class="outline-text-6" id="text-orga3d3475">
<ul class="org-ul">
<li class="on"><code>[X]</code> 1. Download with the article title:</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #FD971F;">sh</span> = SciHub<span style="color: #AE81FF;">()</span>
<span style="color: #FD971F;">title</span> = &#8220;&#8221;&#8220;Improving Traffic Locality <span style="color: #F92672;">in</span> BitTorrent via Biased Neighbor Selection&#8221;&#8220;&#8221;
<span style="color: #FD971F;">meta</span> = sh.find_meta<span style="color: #AE81FF;">(</span>title<span style="color: #AE81FF;">)</span>
<span style="color: #FD971F;">result</span> = sh.download<span style="color: #AE81FF;">(</span>meta.get<span style="color: #66D9EF;">(</span>&#8216;DOI&#8217;<span style="color: #66D9EF;">)</span>, path=title + &#8216;.pdf&#8217;<span style="color: #AE81FF;">)</span>
</pre>
</div>

<ul class="org-ul">
<li class="on"><code>[X]</code> 2. Search for articles on Google Scholar and download them:</li>
</ul>
</div>
</li>

<li><a id="org5774c6d"></a>issues:<br>
<div class="outline-text-6" id="text-org5774c6d">
<ul class="org-ul">
<li class="off"><code>[&#xa0;]</code> captcha验证</li>
<li class="off"><code>[&#xa0;]</code> 上传DOI, pdf到服务器</li>
</ul>
</div>
</li>

<li><a id="org762a09b"></a>twitter account monitoring.<br>
<div class="outline-text-6" id="text-org762a09b">
<p>
monitor AI accounts' newsest tweets, send tweets including the paper mentioned to users daily.
</p>
</div>
</li>

<li><a id="orga8a792c"></a><span class="todo TODO">TODO</span> design a QT UI for the downloader.<br></li>

<li><a id="org37bee89"></a>add tensorflow &amp; estimator into Jupyter notebook.<br>
<div class="outline-text-6" id="text-org37bee89">
<p>
<a href="https://docs.google.com/document/d/1Zm4SsJlHI8cB--E55Kki38QVHclbClZcBtQoaJX4TJU/edit?usp=sharing">https://docs.google.com/document/d/1Zm4SsJlHI8cB--E55Kki38QVHclbClZcBtQoaJX4TJU/edit?usp=sharing</a>
</p>
</div>
</li>
</ul>
</div>

<div id="outline-container-orgd68b8fd" class="outline-5">
<h5 id="orgd68b8fd">machine reading for scientific paper.</h5>
<div class="outline-text-5" id="text-orgd68b8fd">
</div>
<ul class="org-ul">
<li><a id="org1fbabcf"></a>machine reading/comprehension for scientific paper.<br>
<div class="outline-text-6" id="text-org1fbabcf">
<p>
Hermann, K. M., Kočiský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., &amp; Blunsom, P. (2015). Teaching Machines to Read and Comprehend, 1–9. <a href="https://doi.org/10.1109/72.410363">https://doi.org/10.1109/72.410363</a>
</p>
</div>
</li>

<li><a id="orgb1e999d"></a>transfer article summary bullets into questions.<br></li>

<li><a id="orgc9e62bd"></a>event extraction from scientific paper.<br></li>

<li><a id="org7ecd2ac"></a>extract abstract/reference/full-text from scholar paper.<br>
<div class="outline-text-6" id="text-org7ecd2ac">
<p>
Lopez, P., &amp; Romary, L. (2010). HUMB : Automatic Key Term Extraction from Scientific Articles in GROBID. Proceedings of the 5th International Workshop on Semantic Evaluation, (July), 248–251. Retrieved from <a href="http://aclweb.org/anthology/S/S10/S10-1055.pdf">http://aclweb.org/anthology/S/S10/S10-1055.pdf</a>
</p>

<p>
Romary, L., &amp; Lopez, P. (2017). GROBID - Information Extraction from Scientific Publications To cite this version : HAL Id : hal-01673305.
</p>
</div>
</li>

<li><a id="orgc0f5560"></a>extract core claims/sentences.<br>
<div class="outline-text-6" id="text-orgc0f5560">
<p>
Jansen, T., &amp; Kuhn, T. (2017). Extracting core claims from scientific articles. Communications in Computer and Information Science, 765, 32–46. <a href="https://doi.org/10.1007/978-3-319-67468-1_3">https://doi.org/10.1007/978-3-319-67468-1_3</a>
</p>

<p>
Augenstein, I., Das, M., Riedel, S., Vikraman, L., &amp; McCallum, A. (2017). SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications. <a href="https://doi.org/10.18653/v1/S17-2091">https://doi.org/10.18653/v1/S17-2091</a>
</p>
</div>
</li>

<li><a id="orga22820d"></a>information extraction for scientific paper.<br>
<div class="outline-text-6" id="text-orga22820d">
<p>
Chen, J., &amp; Chen, H. (2013). A structured information extraction algorithm for scientific papers based on feature rules learning. Journal of Software, 8(1), 55–62. <a href="https://doi.org/10.4304/jsw.8.1.55-62">https://doi.org/10.4304/jsw.8.1.55-62</a>
</p>

<p>
Ronzano, F., &amp; Saggion, H. (2016). Knowledge extraction and modeling from scientific publications. Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 9792 LNCS, 11–25. <a href="https://doi.org/10.1007/978-3-319-53637-8_2">https://doi.org/10.1007/978-3-319-53637-8_2</a>
</p>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org04eac30" class="outline-2">
<h2 id="org04eac30">Paper Summary</h2>
</div>
</div>
</body>
</html>
